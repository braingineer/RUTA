function [net error] = train_sofnn( data_x, data_y )
%TRAIN_SOFNN On-line learner for a SOFNN
%    Based on Leng, Prasad, and McGinnity(2004)
%    A self-organizing fuzzy neural network is composed of five layers: 
%     1) the input layer.
%     2) the EBF layer, where each node is a fuzzy node that consists of 
%           as many membership functions as the number of inputs.  The
%           membership functions are multiplied to generate an output.
%     3) the normalized layer, where outputs from the previous layer get
%           normalized
%     4) the weighted layer, where the input from the previous layer is
%           multiplied by weights that correspond to the network
%           parameters.
%     5) the output layer, where the outputs from the previous layers get
%           summed up.
%    The learning process includes _structural_ and _parameter_ learning.
%    Structural learning attempts to find an optimal number of nodes in the
%    network, and parameter learning uses Recursive Least Squares method to
%    generate an optimal set of parameters (which correspond to weights in
%    the weighted layer).

    % SUBFUNCTION RUN_NET
    % takes sofnn and an example set of inputs as an input and runs a
    % simulation of the net to generate an output value.
    % also returns the maximum activation value at the EBF layer and the
    % index of the corresponding neuron -- needed to learn the structure
    function [y max_phi best_ebf] = run_net( net, d_x )
        % LAYER 1: INPUT LAYER
        % for each input vector column, it is converted into a matrix such 
        % that there are n repetitions of the input vector across columns.
        % This aids matrix arithmetic and serves no other purpose.
        input = repmat(d_x', 1, net.NumEBFNeurons);
        
        % LAYER 2: EBF LAYER
        % What follows is a complicated-looking, but rather simple arithmetic
        % computation that produces the o/p for each EBF neuron (as a
        % column)
%         disp(d_x')
%         disp(net.MemFunCenters)
%         disp(net.MemFunWidths)
        opEBFNeurons = exp(-1* sum(((input-net.MemFunCenters).^2)...
                                        ./ (2*(net.MemFunWidths.^2))))'; 
        
        % LAYER 3: NORMALIZED LAYER
        % need to normalize the output of the neurons -- divide each by the
        % sum of all outputs
        opNormNeurons = opEBFNeurons ./ (sum(opEBFNeurons));
        
        % LAYER 4: WEIGHTED LAYER
        wts = reshape(net.Params', size(d_x,2)+1, net.NumEBFNeurons)';
        weightedBias = (wts * [1 d_x]')';
        
        % LAYER 5: OUTPUT LAYER
        y = weightedBias * opNormNeurons;
        
        [max_phi best_ebf] = max(opEBFNeurons);
    end

    % SUBFUNCTION UPDATE_WIDTH
    % increases the width of the neuron indicated by the index by a
    % constant factor, till the network performs adequately for the given
    % input
    function [newnet neuronAdded] = update_width( net, id, d_x, d_y )
        newnet = net; % not sure if copy by ref or val. too lazy to check.
        
        for iter = 1:100 % run 20 times
            % the first step is to get the membership function that had the
            % least value for the given input
            MFVals = exp(-1*((d_x' - newnet.MemFunCenters(:,id)).^2) ...
                            ./ (2 * (newnet.MemFunWidths(:, id).^2)) );

            minMF = find(MFVals == min(MFVals));

            % now, update the width
            newnet.MemFunWidths(minMF, id) = k_sigma * newnet.MemFunWidths(minMF, id);

            % check output
            [t_y t_mp ~] = run_net(newnet, d_x);
            
            if abs(t_y - d_y) <= tolerance && t_mp >= 0.1354
                break;
            end
        end
        
        % if error didn't improve, keep old net and add a new node
        if iter == 100
            newnet = add_neuron(net, d_x);
            neuronAdded = true;
        else
            neuronAdded = false;
        end
    end

    % SUBFUNCTION ADD_NEURON
    % adds an EBF neuron to the net
    function newnet = add_neuron( net, d_x )
        newnet = net;
        
        % need minimum distances between each input val and the
        % corresponding centers of membership functions of all neurons
        input = repmat(d_x', 1, newnet.NumEBFNeurons);
        
        [dist indices] = min( abs(input - newnet.MemFunCenters), [], 2 );
        
        % the centers and widths of the membership functions of the new
        % neuron are generated by using rules provided in Leng et al.(2004)
        % The distances generated earlier are compared with minimum
        % allowable separations that are predefined.  If the minimum
        % distance is acceptably close to an existing center, the same
        % center and its corresponding width is used for the corresponding
        % membership function of the new neuron.  Otherwise, the center is
        % set as the corresponding input value and the minimum distance is
        % used as the width.
        newCenters = zeros(size(dist));
        newWidths = zeros(size(dist));
        for iter = 1:size(dist,1)
            if dist(iter) <= k_d(iter)
                newCenters(iter) = newnet.MemFunCenters(iter, indices(iter));
                newWidths(iter)  = newnet.MemFunWidths(iter, indices(iter));
            else
                newCenters(iter) = d_x(iter);
                newWidths(iter)  = dist(iter);
            end
        end
        
        % append the centers and the widths to the network
        newnet.MemFunCenters = [newnet.MemFunCenters newCenters];
        newnet.MemFunWidths  = [newnet.MemFunWidths newWidths];
        newnet.NumEBFNeurons = newnet.NumEBFNeurons + 1;
    end

    % SUBFUNCTION INIT_PARAMS
    % initialize the weights and the Hermitian matrix, Q, using the linear
    % least squares method
    function newnet = init_params(net, data_x, data_y, t)
        newnet = net;
        
        D_t = data_y(1:t);
        
        % brute forcing, because too lazy and _so_ frustrated!
        psi = [];
        for iter=1:t
            % compute the normalized outputs of all neurons for iter-th
            % input. details on how to compute them can be seen in the
            % run_net function.
            d_x = data_x(iter,:);
            input = repmat(d_x', 1, net.NumEBFNeurons);
            opEBFNeurons = exp(-1* sum(((input-net.MemFunCenters).^2)...
                                        ./ (2*(net.MemFunWidths.^2))))';
            opNormNeurons = opEBFNeurons ./ (sum(opEBFNeurons));
        
            % what follows is hard to describe, it's best to look at the
            % paper to understand what's going on.
            ip = [1 d_x];
            % below we have p(i)
            p = [];
            for jter = 1:size(opNormNeurons,1)
                p = [p opNormNeurons(jter)*ip];
            end
            psi = [psi p'];
        end
        
        P_t = psi';
       % newnet.Hermetian = inv(P_t'*P_t);
        %newnet.Params = newnet.Hermetian * P_t' * D_t;
        newnet.Params = pinv(P_t) * D_t;
        
%        disp(newnet.Hermetian);
        %disp(newnet.Params);
    end

    % SUBFUNCTION UPDATE_PARAMS
    % updates the weights and the Hermetian matrix using the recursive
    % least squares algorithm
    function newnet = update_params(net, d_x, d_y, error)
        newnet = net;
        
        % compute p_t
        input = repmat(d_x', 1, net.NumEBFNeurons);
        opEBFNeurons = exp(-1* sum(((input-net.MemFunCenters).^2)...
                                        ./ (2*(net.MemFunWidths.^2))))';
        opNormNeurons = opEBFNeurons ./ (sum(opEBFNeurons));
        
        ip = [1 d_x];
        
        % CAUTION: my p_t is actually p_t' of the document
        p_t = [];
        for jter = 1:size(opNormNeurons,1)
            p_t = [p_t opNormNeurons(jter)*ip];
        end
        
      	if abs(d_y - p_t*newnet.Params) >= error
            L_t = newnet.Hermetian * p_t' * inv(1 + p_t*newnet.Hermetian*p_t');
        
            newnet.Hermetian = (eye(size(p_t,2)) - alpha*L_t*p_t) * newnet.Hermetian;
        
            newnet.Params = newnet.Params + alpha*L_t*(d_y - p_t*newnet.Params);
        end
               
    end

    % ----------------------------- MAIN -------------------------------- %
    % define the output to be a struct
    net = struct;
    
    % we begin...
    numExamples = size(data_x, 1);
    numInputs   = size(data_x, 2); % each column is a feature
    
    % initialize net
    net.NumEBFNeurons = 1;
    
    % other params are taken from Bollen et al. (2010)'s paper.
    tolerance    =  .04; 
    initialWidth =  .01;
    k_sigma      = 1.12;
    k_d          = 0.01 * ones(numInputs,1);
    k_rmse       =  .05;
    
    % for the first example, the centers of the only EBF neuron are made to
    % correspond to the inputs that come in, and the widths are set as all
    % being equal to the initial width
    net.MemFunCenters  = data_x(1,:)';
    net.MemFunWidths   = initialWidth * ones(size(net.MemFunCenters));
    
    % initialize params
    net = init_params(net, data_x, data_y, 1);
    
    error = zeros(numExamples, 1);
    rmse = zeros(numExamples, 1);
    for i=2:numExamples % online algo, updates for each example
        [y max_phi best_ebf] = run_net(net, data_x(i,:));
        
        error(i) = abs(y - data_y(i, :));
                
        %net = update_params(net, data_x(i,:), data_y(i,:), error(i));
        %net = init_params(net, data_x, data_y, i);
        
        if error(i) <= tolerance && max_phi < 0.1354
            % 2nd scenario in Leng et al. (2004)
            % network has good generalization (error is acceptable) but it 
            % doesn't work for the given input.
            % Update the width of the "best" EBF neuron
            %disp('Scenario 2!');
            [net nAdd] = update_width(net, best_ebf, data_x(i,:), data_y(i,:));
           % if nAdd == true
           %     net = init_params(net, data_x, data_y, i);
         %   end
        elseif error(i) > tolerance && max_phi >= 0.1354
            % 3rd scenario in Leng et al. (2004)
            % network has poor generalization, but works for the current
            % input.
            % The solution is to add a new neuron
            %disp('Scenario 3!');
            net = add_neuron(net, data_x(i,:));
          %  net = init_params(net, data_x, data_y, i);
        elseif error(i) > tolerance && max_phi < 0.1354
            % 4th scenario in Leng et al. (2004)
            % network has poor generalization and doesn't work for the
            % given input.
            % The proposed solution is to update "some" of the widths.
            % I'll just update the "best" EBF neuron, the same way I did
            % for the 2nd scenario.  My understanding is that that is what
            % they were suggesting too.
            %disp('Scenario 4!');
            [net nAdd] = update_width(net, best_ebf, data_x(i,:), data_y(i,:));
          %  if nAdd == true
         %       net = init_params(net, data_x, data_y, i);
         %   end
        end
        
        net = init_params(net, data_x, data_y, i);
        
        rmse(i) = sqrt(mean(error));
        %fprintf('i:%f err:%f rmse:%f phi(n):%f #nu     :%f\n',i,error(i),rmse(i),max_phi,net.NumEBFNeurons);
       % disp(net.MemFunCenters);
        %disp(net.MemFunWidths);
        plot(1:numExamples, error);
       % plot(1:numExamples, rmse);
        %pause
    end
end